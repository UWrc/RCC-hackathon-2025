{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b9da5f6-130f-4f88-a65e-eb423545cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246f844-e6e3-44de-9598-8968e3d98f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch uses object-oriented programming and classes to build models. Here, we define a simple convolutional neural network (CNN)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=3):\n",
    "        \"\"\"\n",
    "        Define the layers of the convolutional neural network.\n",
    "\n",
    "        Parameters:\n",
    "            in_channels: int\n",
    "                The number of channels in the input image.             num_classes: int\n",
    "                The number of classes we want to predict, in our case 3\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer: 1 input channel, 8 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        # Max pooling layer: 2x2 window, stride 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Second convolutional layer: 8 input channels, 16 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "            x: torch.Tensor\n",
    "                The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "                The output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\n",
    "        x = self.pool(x)           # Apply max pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\n",
    "        x = self.pool(x)           # Apply max pooling\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)            # Apply fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b46fec25-02e7-4b1e-bee9-47c074bccc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733bba3-fd7a-4bd2-ad45-2453b11d1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate, batch size, and num_epochs can all be changed - see if you can increase the accuracy!\n",
    "\n",
    "input_size = 28*28  # 28x28 pixels (not directly used in CNN)\n",
    "num_classes = 3  \n",
    "learning_rate = 0.001 # \n",
    "batch_size = 64\n",
    "num_epochs = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf5a16-00aa-48df-8f71-0a8bfe2b183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We again use a class to create a Dataset object which we will use to load our images in\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform  # Use what the user passes\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_labels.iloc[idx, 0]\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGBA')  # Ensures 4 channels\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dedcfb-c51a-404f-91a6-673c922285d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We resize our images to be compatible with the model architecture we made\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize to match model expectations\n",
    "    transforms.ToTensor()           # Convert to tensor with shape [C, H, W]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb40472-d733-490e-99c9-e9f0d37b9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we load in our data. Make sure the img_dir path is correct!\n",
    "\n",
    "particle_data = ImageDataset('labels.csv', img_dir='aws_deep_learning/aws_hackathon_data/all_images', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f64c6-7b1b-4620-b579-98ecec84de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a DataLoader object, which will pass our dataset into the model.\n",
    "# Batch size changes the number of images the model sees during each pass, and shuffle ensures they will differ between passes. \n",
    "\n",
    "train_loader = DataLoader(dataset=particle_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a35e9364-5773-4e05-81a6-e86bf609a053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=784, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(in_channels=4, num_classes=num_classes).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "45d1efaa-9e8d-492c-ade2-f25c78b2c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccaa0b-e2ca-4b4e-8229-23434b033c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 14.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#Now we train our model!\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    for batch_index, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "        # Move data and targets to the device (GPU/CPU)\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward pass: compute the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step: update the model parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "36f82d35-2fa7-4bab-805e-ef9b03a72928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 780/1500 with accuracy 52.00%\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of the model on the given dataset loader.\n",
    "\n",
    "    Parameters:\n",
    "        loader: DataLoader\n",
    "            The DataLoader for the dataset to check accuracy on.\n",
    "        model: nn.Module\n",
    "            The neural network model.\n",
    "    \"\"\"\n",
    "    # if loader.dataset.train:\n",
    "    #     print(\"Checking accuracy on training data\")\n",
    "    # else:\n",
    "    #     print(\"Checking accuracy on test data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Forward pass: compute the model output\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)  # Get the index of the max log-probability\n",
    "            num_correct += (predictions == y).sum()  # Count correct predictions\n",
    "            num_samples += predictions.size(0)  # Count total samples\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "        print(f\"Got {num_correct}/{num_samples} with accuracy {accuracy:.2f}%\")\n",
    "    \n",
    "    model.train()  # Set the model back to training mode\n",
    "\n",
    "# Final accuracy check on training and test sets\n",
    "check_accuracy(train_loader, model)\n",
    "#check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cb1ad-f825-4d22-9c2a-96514a76eaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
