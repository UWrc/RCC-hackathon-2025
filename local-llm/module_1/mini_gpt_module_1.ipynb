{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d3c622",
   "metadata": {},
   "source": [
    "# Mini GPT-style Transformer\n",
    "\n",
    "This notebook walks through the implementation of a **minimal GPT-style transformer model**. It's intended for hands-on experimentation and learning about:\n",
    "- Token embeddings\n",
    "- Causal attention masks\n",
    "- Transformer layers\n",
    "- Sampling and generation\n",
    "\n",
    "**Try changing the vocabulary, hidden size, or sampling strategy to see how the model behaves!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e696f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a194489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32\n"
     ]
    }
   ],
   "source": [
    "# Define a larger toy vocabulary\n",
    "vocab = ['<pad>', '<bos>', '<eos>'] + '''hello world how are you i am doing well thanks bye today tomorrow what is your name my nice good bad okay yes no sure maybe friend not really'''.split()\n",
    "stoi = {s: i for i, s in enumerate(vocab)}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07240e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "D_MODEL = 64\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 4\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72efdbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_causal_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "# Show example\n",
    "generate_causal_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068b77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2, attn_weights = self.self_attn(\n",
    "            src, src, src,\n",
    "            attn_mask=src_mask,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False\n",
    "        )\n",
    "        self.attn_weights = attn_weights\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12eb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            MyTransformerEncoderLayer(d_model, n_heads, batch_first=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        x = self.token_embedding(x) + self.pos_embedding[:, :T, :]\n",
    "        mask = generate_causal_mask(T).to(x.device)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask=mask)\n",
    "            self.attn_weights = layer.attn_weights\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8efc1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, prompt, max_new_tokens=5, temperature=1.0):\n",
    "    model.eval()\n",
    "    tokens = torch.tensor([stoi[t] for t in prompt], dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "    for _ in range(max_new_tokens):\n",
    "        if tokens.size(1) > MAX_LEN:\n",
    "            break\n",
    "        logits = model(tokens)\n",
    "        next_token_logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        if itos[next_token.item()] == '<eos>':\n",
    "            break\n",
    "        tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n",
    "    return [itos[i.item()] for i in tokens[0]]\n",
    "\n",
    "\n",
    "def sample_verbose(model, prompt, max_new_tokens=5, temperature=1.0):\n",
    "    model.eval()\n",
    "    tokens = torch.tensor([stoi[t] for t in prompt], dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    steps = []\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if tokens.size(1) > MAX_LEN:\n",
    "            break\n",
    "\n",
    "        logits = model(tokens)\n",
    "        next_token_logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        selected_token = next_token.item()\n",
    "        token_str = itos[selected_token]\n",
    "\n",
    "        # Save step info\n",
    "        steps.append({\n",
    "            'token_index': selected_token,\n",
    "            'token_str': token_str,\n",
    "            'logits': next_token_logits.detach().cpu(),\n",
    "            'probs': probs.detach().cpu(),\n",
    "        })\n",
    "\n",
    "        if token_str == '<eos>':\n",
    "            break\n",
    "\n",
    "        tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "    final_tokens = [itos[i.item()] for i in tokens[0]]\n",
    "    return final_tokens, steps\n",
    "\n",
    "\n",
    "def sample_top_k(model, prompt, max_new_tokens=5, temperature=1.0, top_k=5):\n",
    "    model.eval()\n",
    "    tokens = torch.tensor([stoi[t] for t in prompt], dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    steps = []\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if tokens.size(1) > MAX_LEN:\n",
    "            break\n",
    "\n",
    "        logits = model(tokens)\n",
    "        next_token_logits = logits[0, -1, :] / temperature\n",
    "\n",
    "        # Top-k filtering\n",
    "        topk_logits, topk_indices = torch.topk(next_token_logits, top_k)\n",
    "        topk_probs = F.softmax(topk_logits, dim=-1)\n",
    "        next_token_idx = torch.multinomial(topk_probs, num_samples=1)\n",
    "        selected_token = topk_indices[next_token_idx].item()\n",
    "        token_str = itos[selected_token]\n",
    "\n",
    "        # Save step info\n",
    "        full_probs = torch.zeros_like(next_token_logits)\n",
    "        full_probs[topk_indices] = topk_probs\n",
    "\n",
    "        steps.append({\n",
    "            'token_index': selected_token,\n",
    "            'token_str': token_str,\n",
    "            'logits': next_token_logits.detach().cpu(),\n",
    "            'probs': full_probs.detach().cpu(),  # filled with zeros except top-k\n",
    "        })\n",
    "\n",
    "        if token_str == '<eos>':\n",
    "            break\n",
    "\n",
    "        tokens = torch.cat([tokens, torch.tensor([[selected_token]], device=DEVICE)], dim=1)\n",
    "\n",
    "    final_tokens = [itos[i.item()] for i in tokens[0]]\n",
    "    return final_tokens, steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87568137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generation_steps(steps, top_k=10):\n",
    "    for i, step in enumerate(steps):\n",
    "        logits = step['logits']\n",
    "        probs = step['probs']\n",
    "        df = pd.DataFrame({\n",
    "            'token': [itos[j] for j in range(len(logits))],\n",
    "            'logits': logits.tolist(),\n",
    "            'probs': probs.tolist()\n",
    "        }).sort_values('probs', ascending=False).head(top_k)\n",
    "\n",
    "        print(f\"\\nStep {i + 1}: Generated token = `{step['token_str']}`\")\n",
    "        display(df)\n",
    "\n",
    "        df.plot(x='token', y=['logits', 'probs'], kind='bar', figsize=(8, 4))\n",
    "        plt.grid(True)\n",
    "        plt.title(f\"Step {i+1}: Logits vs Probs\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0793a5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: ['<bos>', 'hello', 'my', 'maybe', 'well', 'are', 'no']\n"
     ]
    }
   ],
   "source": [
    "model = MiniGPT(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS, MAX_LEN).to(DEVICE)\n",
    "prompt = ['<bos>', 'hello']\n",
    "final_output = sample(model, prompt, max_new_tokens=5, temperature=1.0)\n",
    "final_output, steps = sample_top_k(model, prompt, max_new_tokens=5, temperature=1.0, top_k=1)\n",
    "final_output, steps = sample_verbose(model, prompt, max_new_tokens=5, temperature=0.01)\n",
    "print(\"Generated:\", final_output)\n",
    "# show_generation_steps(steps, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
